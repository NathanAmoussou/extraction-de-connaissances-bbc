{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82be1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = Path(\"bbc_news_train.csv\")\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Impossible de trouver le fichier : {DATA_PATH.resolve()}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b913fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1552</td>\n",
       "      <td>moving mobile improves golf swing a mobile pho...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>405</td>\n",
       "      <td>bt boosts its broadband packages british telec...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>702</td>\n",
       "      <td>peer-to-peer nets  here to stay  peer-to-peer ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1951</td>\n",
       "      <td>pompeii gets digital make-over the old-fashion...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ArticleId                                               Text Category\n",
       "3        1976  lifestyle  governs mobile choice  faster  bett...     tech\n",
       "19       1552  moving mobile improves golf swing a mobile pho...     tech\n",
       "24        405  bt boosts its broadband packages british telec...     tech\n",
       "26        702  peer-to-peer nets  here to stay  peer-to-peer ...     tech\n",
       "30       1951  pompeii gets digital make-over the old-fashion...     tech"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles dans le dataset filtré : 10\n"
     ]
    }
   ],
   "source": [
    "# df_tech = df[df[\"Category\"] == \"tech\"].head(100).copy()\n",
    "df_tech = df[df[\"Category\"] == \"tech\"].head(10).copy()\n",
    "display(df_tech.head())\n",
    "print(f\"Nombre d'articles dans le dataset filtré : {len(df_tech)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# petit set de relations pour éviter un graphe incohérent\n",
    "\n",
    "REL_SCHEMA = [\n",
    "  {\"pred\":\"schema:about\",\n",
    "   \"subj\":[\"schema:NewsArticle\"], \"obj\":[\"schema:Thing\"],\n",
    "   \"note\":\"Article -> entités mentionnées/centrales\"},\n",
    "\n",
    "  {\"pred\":\"schema:author\",\n",
    "   \"subj\":[\"schema:NewsArticle\"], \"obj\":[\"foaf:Person\",\"schema:Organization\"],\n",
    "   \"note\":\"Optionnel (souvent absent) : auteur ou média\"},\n",
    "\n",
    "  {\"pred\":\"schema:worksFor\",\n",
    "   \"subj\":[\"foaf:Person\"], \"obj\":[\"schema:Organization\"],\n",
    "   \"note\":\"Emploi / affiliation forte\"},\n",
    "\n",
    "  {\"pred\":\"schema:founder\",\n",
    "   \"subj\":[\"schema:Organization\"], \"obj\":[\"foaf:Person\"],\n",
    "   \"note\":\"Organisation -> fondateur(s)\"},\n",
    "\n",
    "  {\"pred\":\"schema:acquiredBy\",\n",
    "   \"subj\":[\"schema:Organization\"], \"obj\":[\"schema:Organization\"],\n",
    "   \"note\":\"Convention: acquis -> acquéreur\"},\n",
    "\n",
    "  {\"pred\":\"schema:produces\",\n",
    "   \"subj\":[\"schema:Organization\"], \"obj\":[\"schema:Product\"],\n",
    "   \"note\":\"Entreprise -> produit (hardware/software)\"},\n",
    "\n",
    "  {\"pred\":\"schema:location\",\n",
    "   \"subj\":[\"schema:Organization\",\"schema:Event\"], \"obj\":[\"schema:Place\"],\n",
    "   \"note\":\"Si une localisation est explicitement mentionnée\"},\n",
    "\n",
    "  {\"pred\":\"onto:announced\",\n",
    "   \"subj\":[\"schema:Organization\",\"foaf:Person\"], \"obj\":[\"schema:Product\",\"schema:Event\"],\n",
    "   \"note\":\"Local (si tu veux expliciter les annonces/lancements)\"},\n",
    "\n",
    "  {\"pred\":\"schema:datePublished\",\n",
    "   \"subj\":[\"schema:NewsArticle\"], \"obj\":[\"xsd:date\",\"xsd:dateTime\"],\n",
    "   \"note\":\"Attribut date article (si dispo / inférée)\"},\n",
    "\n",
    "  {\"pred\":\"schema:releaseDate\",\n",
    "   \"subj\":[\"schema:Product\",\"schema:Event\"], \"obj\":[\"xsd:date\",\"xsd:dateTime\"],\n",
    "   \"note\":\"Attribut date sortie/lancement (si mentionné)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f73831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_TYPES = {\n",
    "  \"schema:worksFor\":   ({\"PERSON\"}, {\"ORG\"}),\n",
    "  \"schema:founder\":    ({\"ORG\"}, {\"PERSON\"}),\n",
    "  \"schema:acquiredBy\": ({\"ORG\"}, {\"ORG\"}),\n",
    "  \"schema:produces\":   ({\"ORG\"}, {\"PRODUCT\"}),\n",
    "  \"schema:location\":   ({\"ORG\",\"EVENT\"}, {\"GPE\"}),\n",
    "  \"onto:announced\":    ({\"ORG\",\"PERSON\"}, {\"PRODUCT\",\"EVENT\"}),\n",
    "  # about/author/datePublished/releaseDate : laisse plus souple pour l’instant\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c13319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Dict, Any, Tuple\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# Détection \"best effort\" des colonnes\n",
    "TEXT_COL = next((c for c in [\"Text\", \"text\", \"News\", \"news\", \"Content\", \"content\"] if c in df_tech.columns), None)\n",
    "if TEXT_COL is None:\n",
    "    raise KeyError(f\"Colonne texte introuvable. Colonnes dispo: {list(df_tech.columns)}\")\n",
    "\n",
    "ID_COL = next((c for c in [\"id\", \"Id\", \"ID\", \"article_id\", \"ArticleId\"] if c in df_tech.columns), None)\n",
    "\n",
    "ALLOWED_PREDS = [x[\"pred\"] for x in REL_SCHEMA]\n",
    "\n",
    "CACHE_DIR = Path(\"cache_extractions\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393210d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_JSON_SCHEMA = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"entities\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"mention\": {\"type\": \"string\", \"minLength\": 1},\n",
    "          \"type\": {\"type\": \"string\", \"enum\": [\"PERSON\",\"ORG\",\"GPE\",\"PRODUCT\",\"EVENT\"]},\n",
    "          \"start\": {\"type\": \"integer\"},\n",
    "          \"end\": {\"type\": \"integer\"}\n",
    "        },\n",
    "        \"required\": [\"mention\",\"type\",\"start\",\"end\"],\n",
    "        \"additionalProperties\": False\n",
    "      }\n",
    "    },\n",
    "    \"relations\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"subj\": {\"type\": \"string\", \"minLength\": 1},\n",
    "          \"pred\": {\"type\": \"string\", \"enum\": ALLOWED_PREDS},\n",
    "          \"obj\": {\"type\": \"string\", \"minLength\": 1},\n",
    "          \"evidence\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"subj\",\"pred\",\"obj\",\"evidence\"],\n",
    "        \"additionalProperties\": False\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"entities\",\"relations\"],\n",
    "  \"additionalProperties\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c8be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "EntityType = Literal[\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"EVENT\"]\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    mention: str = Field(min_length=1)\n",
    "    type: EntityType\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    subj: str = Field(min_length=1)\n",
    "    pred: str = Field(min_length=1)\n",
    "    obj: str = Field(min_length=1)\n",
    "    evidence: str = \"\"\n",
    "\n",
    "class Extraction(BaseModel):\n",
    "    entities: List[Entity] = []\n",
    "    relations: List[Relation] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5c4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s*```$\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _extract_first_json_object(s: str) -> str:\n",
    "    # essaie de récupérer le premier bloc {...} si le modèle a bavé\n",
    "    s = _strip_code_fences(s)\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return s\n",
    "    return s[start:end+1]\n",
    "\n",
    "def _find_span(text: str, mention: str):\n",
    "    if not mention:\n",
    "        return -1, -1, mention\n",
    "    # cherche case-insensitive mais renvoie les indices exacts\n",
    "    m = re.search(re.escape(mention), text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return -1, -1, mention\n",
    "    real = text[m.start():m.end()]  # substring exact du texte (casse d'origine)\n",
    "    return m.start(), m.end(), real\n",
    "\n",
    "def _postprocess(extr: Extraction, text: str) -> Extraction:\n",
    "    # offsets\n",
    "    fixed_entities = []\n",
    "    for e in extr.entities:\n",
    "        s, t, real = _find_span(text, e.mention)\n",
    "        fixed_entities.append(Entity(mention=real, type=e.type, start=s, end=t))\n",
    "    extr.entities = fixed_entities\n",
    "\n",
    "\n",
    "    entity_mentions = {e.mention for e in extr.entities}\n",
    "\n",
    "    # relations: pred whitelist + subj/obj exist + evidence substring\n",
    "    fixed_rel = []\n",
    "    for r in extr.relations:\n",
    "        if r.pred not in ALLOWED_PREDS:\n",
    "            continue\n",
    "        if (r.subj not in entity_mentions) or (r.obj not in entity_mentions and r.pred not in [\"schema:datePublished\", \"schema:releaseDate\"]):\n",
    "            # pour les dates, obj peut être un literal ISO. Sinon on exige obj dans entities.\n",
    "            continue\n",
    "        if r.evidence and (r.evidence not in text):\n",
    "            r.evidence = \"\"\n",
    "        fixed_rel.append(r)\n",
    "\n",
    "    extr.relations = fixed_rel\n",
    "    return extr\n",
    "\n",
    "# ============================================================\n",
    "# A) Corriger les mentions : récupérer la casse réelle + dédoublonner\n",
    "# ============================================================\n",
    "\n",
    "def find_span_case_insensitive(text: str, mention: str) -> Tuple[int, int, str]:\n",
    "    \"\"\"Trouve la mention dans le texte (case-insensitive) et retourne la casse réelle.\"\"\"\n",
    "    if not mention:\n",
    "        return -1, -1, mention\n",
    "    m = re.search(re.escape(mention), text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return -1, -1, mention\n",
    "    real = text[m.start():m.end()]  # substring exact (casse d'origine)\n",
    "    return m.start(), m.end(), real\n",
    "\n",
    "def dedup_entities(entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Dédoublonne les entités par (type, start, end) si start!=-1, sinon (type, mention.lower()).\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for e in entities:\n",
    "        key = (e[\"type\"], e[\"start\"], e[\"end\"]) if e[\"start\"] != -1 else (e[\"type\"], e[\"mention\"].lower())\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(e)\n",
    "    return out\n",
    "\n",
    "def fix_entities_from_text(extr: Dict[str, Any], text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Corrige les mentions (casse réelle) et dédoublonne les entités.\"\"\"\n",
    "    ents = []\n",
    "    for e in extr.get(\"entities\", []):\n",
    "        s, t, real = find_span_case_insensitive(text, e[\"mention\"])\n",
    "        ents.append({**e, \"mention\": real, \"start\": s, \"end\": t})\n",
    "    extr[\"entities\"] = dedup_entities(ents)\n",
    "    return extr\n",
    "\n",
    "# ============================================================\n",
    "# B) Nettoyer / retourner les relations + vérifier l'evidence\n",
    "# ============================================================\n",
    "\n",
    "def clean_relations(extr: Dict[str, Any], text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Nettoie les relations :\n",
    "    - Retourne automatiquement les relations inversées (ex: worksFor Person→Org)\n",
    "    - Rejette les relations dont les types ne matchent pas le schéma\n",
    "    - Exige que l'evidence contienne les deux mentions (subj+obj)\n",
    "    \"\"\"\n",
    "    types = {e[\"mention\"]: e[\"type\"] for e in extr.get(\"entities\", [])}\n",
    "    mentions = set(types.keys())\n",
    "\n",
    "    def has_both_in_evidence(subj: str, obj: str, ev: str) -> bool:\n",
    "        if not ev:\n",
    "            return False\n",
    "        # case-insensitive containment\n",
    "        return (re.search(re.escape(subj), ev, re.IGNORECASE) is not None) and \\\n",
    "               (re.search(re.escape(obj), ev, re.IGNORECASE) is not None)\n",
    "\n",
    "    cleaned = []\n",
    "    for r in extr.get(\"relations\", []):\n",
    "        pred = r.get(\"pred\")\n",
    "        subj = r.get(\"subj\")\n",
    "        obj  = r.get(\"obj\")\n",
    "        ev   = r.get(\"evidence\", \"\") or \"\"\n",
    "\n",
    "        if pred in PRED_TYPES:\n",
    "            if not ev:  # pas de preuve => on drop\n",
    "                continue\n",
    "        if pred not in ALLOWED_PREDS:\n",
    "            continue\n",
    "        if subj not in mentions:\n",
    "            continue\n",
    "        if pred not in [\"schema:datePublished\", \"schema:releaseDate\"] and obj not in mentions:\n",
    "            continue\n",
    "\n",
    "        # evidence doit être substring du texte (sinon vide)\n",
    "        if ev and ev not in text:\n",
    "            ev = \"\"\n",
    "\n",
    "        if pred == \"schema:releaseDate\":\n",
    "            if types.get(subj) not in {\"PRODUCT\",\"EVENT\"}:\n",
    "                continue\n",
    "\n",
    "        # typing + auto-swap si inversé\n",
    "        if pred in PRED_TYPES and obj in mentions:\n",
    "            subj_ok, obj_ok = PRED_TYPES[pred]\n",
    "            st = types.get(subj)\n",
    "            ot = types.get(obj)\n",
    "\n",
    "            # swap si inversé\n",
    "            if (st in obj_ok) and (ot in subj_ok):\n",
    "                subj, obj = obj, subj\n",
    "                st, ot = ot, st\n",
    "\n",
    "            if not (st in subj_ok and ot in obj_ok):\n",
    "                continue\n",
    "\n",
    "            # evidence doit contenir les 2 mentions (sinon on drop)\n",
    "            if not has_both_in_evidence(subj, obj, ev):\n",
    "                continue\n",
    "\n",
    "        cleaned.append({\"subj\": subj, \"pred\": pred, \"obj\": obj, \"evidence\": ev})\n",
    "\n",
    "    extr[\"relations\"] = cleaned\n",
    "    return extr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46352bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"llama3.1:8b\"\n",
    "\n",
    "def _ollama_chat(messages: List[Dict[str, str]], temperature: float = 0.0, timeout: int = 120) -> str:\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "        \"format\": EXTRACTION_JSON_SCHEMA,\n",
    "        \"options\": {\"temperature\": temperature}\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"message\"][\"content\"]\n",
    "\n",
    "def _build_prompt(article_text: str) -> List[Dict[str, str]]:\n",
    "    system = (\n",
    "        \"Tu es un extracteur d'information. \"\n",
    "        \"Tu DOIS répondre en JSON strict, sans aucun texte autour, sans markdown. \"\n",
    "        \"Le JSON doit respecter EXACTEMENT ce schéma: \"\n",
    "        \"{\\\"entities\\\":[{\\\"mention\\\":str,\\\"type\\\":\\\"PERSON|ORG|GPE|PRODUCT|EVENT\\\",\\\"start\\\":int,\\\"end\\\":int}],\"\n",
    "        \"\\\"relations\\\":[{\\\"subj\\\":str,\\\"pred\\\":str,\\\"obj\\\":str,\\\"evidence\\\":str}]}. \"\n",
    "        f\"pred doit être dans cette liste exacte: {ALLOWED_PREDS}. \"\n",
    "        \"Règles: \"\n",
    "        \"1) Les champs entities/relations existent toujours (liste vide ok). \"\n",
    "        \"2) subj et obj doivent correspondre à des 'mention' présentes dans entities (sauf pred datePublished/releaseDate où obj peut être une date ISO). \"\n",
    "        \"3) evidence doit être un extrait COPIÉ du texte source (30-200 chars si possible). \"\n",
    "        \"4) N'invente rien: si doute -> n'extrais pas.\"\n",
    "    )\n",
    "\n",
    "    user = (\n",
    "        \"Texte:\\n\"\n",
    "        f\"{article_text}\\n\\n\"\n",
    "        \"Retourne UNIQUEMENT le JSON.\"\n",
    "    )\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "def validate_extraction_loose(parsed: dict) -> Extraction:\n",
    "    ents = []\n",
    "    for e in parsed.get(\"entities\", []) or []:\n",
    "        try:\n",
    "            ents.append(Entity.model_validate(e))\n",
    "        except ValidationError:\n",
    "            pass\n",
    "\n",
    "    rels = []\n",
    "    for r in parsed.get(\"relations\", []) or []:\n",
    "        # skip si champs essentiels vides\n",
    "        if not r.get(\"subj\") or not r.get(\"pred\") or not r.get(\"obj\"):\n",
    "            continue\n",
    "        try:\n",
    "            rels.append(Relation.model_validate(r))\n",
    "        except ValidationError:\n",
    "            pass\n",
    "\n",
    "    return Extraction(entities=ents, relations=rels)\n",
    "\n",
    "def extract(article_text: str, article_id: str | int, max_retries: int = 3) -> Dict[str, Any]:\n",
    "    cache_path = CACHE_DIR / f\"{article_id}.jsonl\"\n",
    "    if cache_path.exists():\n",
    "        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.loads(f.readline())\n",
    "\n",
    "    messages = _build_prompt(article_text)\n",
    "\n",
    "    last_err = None\n",
    "    raw = \"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        raw = _ollama_chat(messages, temperature=0.0)\n",
    "        raw_json = _extract_first_json_object(raw)\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw_json)\n",
    "            # extr = Extraction.model_validate(parsed)\n",
    "            extr = validate_extraction_loose(parsed)\n",
    "            extr = _postprocess(extr, article_text)\n",
    "\n",
    "            out = extr.model_dump()\n",
    "            # cache (jsonl = une ligne)\n",
    "            with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "            return out\n",
    "\n",
    "        except (json.JSONDecodeError, ValidationError) as e:\n",
    "            last_err = str(e)\n",
    "            # retry: demander une correction JSON stricte\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Corrige la sortie pour qu'elle soit du JSON strict conforme au schéma. Réponds en JSON uniquement.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"ERREUR:\\n{last_err}\\n\\nSORTIE A CORRIGER:\\n{raw}\\n\\nRappel: pred doit être dans {ALLOWED_PREDS}.\"}\n",
    "            ]\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # si échec total: on cache un résultat vide (évite boucles)\n",
    "    fallback = {\"entities\": [], \"relations\": [], \"error\": last_err, \"raw\": raw[:1000]}\n",
    "    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(fallback, ensure_ascii=False) + \"\\n\")\n",
    "    return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11be5543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:16<00:00, 13.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['article_id', 'entities', 'relations']), 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in tqdm(df_tech.iterrows(), total=len(df_tech)):\n",
    "    article_id = row[ID_COL] if ID_COL else i\n",
    "    text = str(row[TEXT_COL])\n",
    "\n",
    "    res = extract(text, article_id=article_id)\n",
    "\n",
    "    # Post-traitement : correction casse + déduplication + nettoyage relations\n",
    "    res = fix_entities_from_text(res, text)\n",
    "    res = clean_relations(res, text)\n",
    "\n",
    "    results.append({\"article_id\": article_id, **res})\n",
    "\n",
    "results[0].keys(), len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ky8veztoxf",
   "metadata": {},
   "source": [
    "# 5) NED — Désambiguïsation intra-corpus\n",
    "\n",
    "Normalisation des mentions et construction de noms canoniques pour regrouper les variantes (IBM vs I.B.M., etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rk59lfvcol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, ['ORG', 'PERSON', 'PRODUCT', 'GPE', 'EVENT'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1. Normalisation + collecte des mentions\n",
    "from collections import Counter, defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    \"\"\"Normalise un nom pour la comparaison (minuscule, sans ponctuation).\"\"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[\\W_]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# On part de `results` (un élément par article avec \"article_id\", \"entities\", \"relations\")\n",
    "mention_freq = Counter()\n",
    "mentions_by_type = defaultdict(set)\n",
    "\n",
    "for r in results:\n",
    "    for e in r.get(\"entities\", []):\n",
    "        m = e[\"mention\"]\n",
    "        t = e[\"type\"]\n",
    "        if not m or len(m) < 2:\n",
    "            continue\n",
    "        mention_freq[(t, m)] += 1\n",
    "        mentions_by_type[t].add(m)\n",
    "\n",
    "len(mention_freq), list(mentions_by_type.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4wnvqw72dvi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2. Construire des \"canonical names\" par type (IBM vs I.B.M.)\n",
    "\n",
    "def build_canonical_map(threshold: int = 92):\n",
    "    \"\"\"\n",
    "    Retourne un dict (type, mention) -> canonical_mention\n",
    "    threshold : score RapidFuzz au-dessus duquel deux mentions sont fusionnées\n",
    "    \"\"\"\n",
    "    canon_map = {}\n",
    "    for t, mentions in mentions_by_type.items():\n",
    "        # plus fréquent d'abord (devient souvent le canon)\n",
    "        sorted_mentions = sorted(list(mentions),\n",
    "                                 key=lambda m: mention_freq[(t, m)],\n",
    "                                 reverse=True)\n",
    "        reps = []  # représentants canoniques\n",
    "\n",
    "        for m in sorted_mentions:\n",
    "            nm = norm_name(m)\n",
    "            best_rep = None\n",
    "            best_score = -1\n",
    "\n",
    "            for rep in reps:\n",
    "                score = fuzz.ratio(nm, norm_name(rep))\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_rep = rep\n",
    "\n",
    "            if best_score >= threshold:\n",
    "                # proche d'un représentant existant\n",
    "                canon_map[(t, m)] = best_rep\n",
    "            else:\n",
    "                # nouveau représentant canonique\n",
    "                reps.append(m)\n",
    "                canon_map[(t, m)] = m\n",
    "    return canon_map\n",
    "\n",
    "CANON_MAP = build_canonical_map(threshold=92)\n",
    "len(CANON_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29f19f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def acronym(s: str) -> str:\n",
    "    # \"British Telecom\" -> \"BT\"\n",
    "    words = re.findall(r\"[A-Za-z]+\", s)\n",
    "    if len(words) < 2:\n",
    "        return \"\"\n",
    "    return \"\".join(w[0].upper() for w in words)\n",
    "\n",
    "# Merge acronyme -> forme longue (ORG seulement)\n",
    "# Exemple: \"BT\" devient canonique \"British Telecom\" si présent dans le corpus\n",
    "for (t, m), canon in list(CANON_MAP.items()):\n",
    "    if t != \"ORG\":\n",
    "        continue\n",
    "\n",
    "    short = m.strip()\n",
    "    if len(short) > 5:\n",
    "        continue  # on ne considère que les entités très courtes (BT, IBM, …)\n",
    "\n",
    "    for long_m in mentions_by_type[\"ORG\"]:\n",
    "        if len(long_m) < 8:\n",
    "            continue  # ignore formes trop courtes\n",
    "        if acronym(long_m) == short.upper():\n",
    "            # force le canon de \"BT\" vers le canon de \"British Telecom\"\n",
    "            CANON_MAP[(t, m)] = CANON_MAP.get((t, long_m), long_m)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yk2mm382qq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ericsson  ->  ericsson ( ORG )\n",
      "dr michael bjorn  ->  dr michael bjorn ( PERSON )\n",
      "bbc news website  ->  bbc news website ( PRODUCT )\n",
      "japan  ->  japan ( GPE )\n",
      "uk  ->  uk ( GPE )\n"
     ]
    }
   ],
   "source": [
    "# 5.3. Appliquer la canonicalisation dans results\n",
    "\n",
    "# On ajoute un champ \"canonical\" à chaque entité\n",
    "for r in results:\n",
    "    ents = []\n",
    "    for e in r.get(\"entities\", []):\n",
    "        key = (e[\"type\"], e[\"mention\"])\n",
    "        canonical = CANON_MAP.get(key, e[\"mention\"])\n",
    "        ents.append({**e, \"canonical\": canonical})\n",
    "    r[\"entities\"] = ents\n",
    "\n",
    "# petit check\n",
    "for e in results[0][\"entities\"][:5]:\n",
    "    print(e[\"mention\"], \" -> \", e[\"canonical\"], \"(\", e[\"type\"], \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyjkngsqdm",
   "metadata": {},
   "source": [
    "# 6) NEL — Lier les canoniques à Wikidata\n",
    "\n",
    "On va :\n",
    "1. Créer un cache NEL pour ne pas spammer Wikidata\n",
    "2. Faire une requête SPARQL simple par label + type\n",
    "3. Choisir un candidat (heuristique simple, top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da9augjzb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Préparer le cache + mapping des types vers Wikidata\n",
    "\n",
    "NEL_CACHE_DIR = Path(\"cache_nel\")\n",
    "NEL_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Mapping type -> QID générique Wikidata\n",
    "WIKIDATA_TYPE_QID = {\n",
    "    \"PERSON\": \"Q5\",          # human\n",
    "    \"ORG\": \"Q43229\",         # organization\n",
    "    \"GPE\": \"Q17334923\",      # geographic location\n",
    "    \"PRODUCT\": \"Q2424752\",   # product\n",
    "    \"EVENT\": \"Q1656682\",     # event\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "i24s6k96ix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "# session avec retries + backoff (utile si 429 / 503)\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=6,\n",
    "    backoff_factor=1.0,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    "    respect_retry_after_header=True,\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"td3-kg/1.0 (student project; contact: your_email_or_name)\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "def wikidata_candidates(label: str, ent_type: str, limit: int = 5):\n",
    "    \"\"\"\n",
    "    Retourne [{uri,label,desc,id}] via wbsearchentities.\n",
    "    ent_type gardé pour la clé de cache (et éventuellement filtrage plus tard).\n",
    "    \"\"\"\n",
    "    key = f\"{ent_type}__{norm_name(label)}.json\"\n",
    "    cache_path = NEL_CACHE_DIR / key\n",
    "    if cache_path.exists():\n",
    "        return json.loads(cache_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if not label or not label.strip():\n",
    "        return []\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": label.strip(),\n",
    "        \"language\": \"en\",\n",
    "        \"limit\": limit,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    resp = session.get(WIKIDATA_API_URL, params=params, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    out = []\n",
    "    for s in data.get(\"search\", []):\n",
    "        out.append({\n",
    "            \"uri\": s.get(\"concepturi\"),       # URL Wikidata de l'item (URI)\n",
    "            \"id\": s.get(\"id\"),                # Qxxxx\n",
    "            \"label\": s.get(\"label\", \"\"),\n",
    "            \"desc\": s.get(\"description\", \"\"),\n",
    "        })\n",
    "\n",
    "    cache_path.write_text(json.dumps(out, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    time.sleep(0.2)  # politesse (Wikidata recommande de ne pas spam) :contentReference[oaicite:3]{index=3}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "oa9kah0d93j",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acronym_label(s: str) -> str:\n",
    "    words = re.findall(r\"[A-Za-z]+\", s)\n",
    "    if len(words) < 2:\n",
    "        return \"\"\n",
    "    return \"\".join(w[0].upper() for w in words)\n",
    "\n",
    "def token_set(s: str) -> set:\n",
    "    return set(norm_name(s).split())\n",
    "\n",
    "def pick_best_candidate(label: str, ent_type: str, candidates: list):\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    nl = norm_name(label)\n",
    "    short = len(nl) <= 3  # ex: uk, bt, ibm\n",
    "\n",
    "    # 1) Abréviations: cherche un candidat dont l'acronyme matche (UK -> United Kingdom)\n",
    "    if short:\n",
    "        for c in candidates:\n",
    "            ac = acronym_label(c.get(\"label\", \"\"))\n",
    "            if ac and norm_name(ac) == nl:\n",
    "                return c\n",
    "\n",
    "        # 1bis) Pour GPE abréviation: préfère un \"country\" si présent\n",
    "        if ent_type == \"GPE\":\n",
    "            for c in candidates:\n",
    "                d = (c.get(\"desc\") or \"\").lower()\n",
    "                if \"country\" in d:\n",
    "                    return c\n",
    "\n",
    "        # sinon: on ne prend PAS candidates[0] (souvent faux comme Ukrainian)\n",
    "        # fallback: meilleur fuzzy sur label\n",
    "        best = max(candidates, key=lambda c: fuzz.ratio(nl, norm_name(c.get(\"label\",\"\"))))\n",
    "        return best\n",
    "\n",
    "    # 2) Score combiné fuzzy + overlap tokens + bonus type\n",
    "    qtokens = token_set(label)\n",
    "\n",
    "    def score(c):\n",
    "        cl = c.get(\"label\", \"\")\n",
    "        cd = (c.get(\"desc\") or \"\").lower()\n",
    "        s1 = fuzz.ratio(nl, norm_name(cl))\n",
    "        s2 = int(100 * (len(qtokens & token_set(cl)) / max(1, len(qtokens))))\n",
    "        bonus = 0\n",
    "        if ent_type == \"GPE\" and \"country\" in cd:\n",
    "            bonus += 10\n",
    "        if ent_type == \"ORG\" and (\"company\" in cd or \"telecom\" in cd or \"provider\" in cd):\n",
    "            bonus += 10\n",
    "        return 0.7 * s1 + 0.3 * s2 + bonus\n",
    "\n",
    "    best = max(candidates, key=score)\n",
    "\n",
    "    # seuil léger (TD-friendly)\n",
    "    best_score = score(best)\n",
    "    if best_score < (75 if ent_type == \"PERSON\" else 55):\n",
    "        return None\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "klef224a4wq",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEL Wikidata: 100%|██████████| 71/71 [00:03<00:00, 19.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.4. Construire une table canonical -> uri (NEL)\n",
    "\n",
    "# On lie uniquement les entités :\n",
    "# - avec start != -1\n",
    "# - dont la canonical apparaît au moins 2 fois dans le corpus (moins de bruit)\n",
    "# - de type PERSON / ORG / GPE (les plus \"faciles\")\n",
    "\n",
    "# 1) fréquence des canoniques\n",
    "canon_freq = Counter()\n",
    "for r in results:\n",
    "    for e in r.get(\"entities\", []):\n",
    "        if e[\"start\"] == -1:\n",
    "            continue\n",
    "        canon = e.get(\"canonical\") or e[\"mention\"]\n",
    "        canon_freq[(e[\"type\"], canon)] += 1\n",
    "\n",
    "# 2) NEL\n",
    "entity_links = {}  # (type, canonical) -> uri\n",
    "\n",
    "for (t, canon), f in tqdm(canon_freq.items(), desc=\"NEL Wikidata\"):\n",
    "    if t not in {\"PERSON\", \"ORG\", \"GPE\"}:\n",
    "        continue\n",
    "    if f < 2:\n",
    "        continue  # trop rare, on saute\n",
    "\n",
    "    cands = wikidata_candidates(canon, t, limit=5)\n",
    "    chosen = pick_best_candidate(canon, t, cands)\n",
    "    if chosen:\n",
    "        entity_links[(t, canon)] = chosen[\"uri\"]\n",
    "\n",
    "len(entity_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hv0g8qdx4r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ericsson  | canonical: ericsson  | uri: None\n",
      "dr michael bjorn  | canonical: dr michael bjorn  | uri: None\n",
      "bbc news website  | canonical: bbc news website  | uri: None\n",
      "japan  | canonical: japan  | uri: http://www.wikidata.org/entity/Q17\n",
      "uk  | canonical: uk  | uri: http://www.wikidata.org/entity/Q145\n"
     ]
    }
   ],
   "source": [
    "# 6.5. Ajouter l'URI dans les entités\n",
    "\n",
    "for r in results:\n",
    "    ents = []\n",
    "    for e in r.get(\"entities\", []):\n",
    "        canon = e.get(\"canonical\") or e[\"mention\"]\n",
    "        uri = entity_links.get((e[\"type\"], canon))\n",
    "        ents.append({**e, \"uri\": uri})\n",
    "    r[\"entities\"] = ents\n",
    "\n",
    "# check rapide\n",
    "for e in results[0][\"entities\"]:\n",
    "    print(e[\"mention\"], \" | canonical:\", e[\"canonical\"], \" | uri:\", e.get(\"uri\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c09008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
