{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82be1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = Path(\"bbc_news_train.csv\")\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Impossible de trouver le fichier : {DATA_PATH.resolve()}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b913fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1552</td>\n",
       "      <td>moving mobile improves golf swing a mobile pho...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>405</td>\n",
       "      <td>bt boosts its broadband packages british telec...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>702</td>\n",
       "      <td>peer-to-peer nets  here to stay  peer-to-peer ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1951</td>\n",
       "      <td>pompeii gets digital make-over the old-fashion...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ArticleId                                               Text Category\n",
       "3        1976  lifestyle  governs mobile choice  faster  bett...     tech\n",
       "19       1552  moving mobile improves golf swing a mobile pho...     tech\n",
       "24        405  bt boosts its broadband packages british telec...     tech\n",
       "26        702  peer-to-peer nets  here to stay  peer-to-peer ...     tech\n",
       "30       1951  pompeii gets digital make-over the old-fashion...     tech"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles dans le dataset filtré : 100\n"
     ]
    }
   ],
   "source": [
    "df_tech = df[df[\"Category\"] == \"tech\"].head(100).copy()\n",
    "display(df_tech.head())\n",
    "print(f\"Nombre d'articles dans le dataset filtré : {len(df_tech)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# petit set de relations pour éviter un graphe incohérent\n",
    "\n",
    "REL_SCHEMA = [\n",
    "  {\"pred\":\"schema:about\",\n",
    "   \"subj\":[\"schema:NewsArticle\"], \"obj\":[\"schema:Thing\"],\n",
    "   \"note\":\"Article -> entités mentionnées/centrales\"},\n",
    "\n",
    "  {\"pred\":\"schema:author\",\n",
    "   \"subj\":[\"schema:NewsArticle\"], \"obj\":[\"foaf:Person\",\"schema:Organization\"],\n",
    "   \"note\":\"Optionnel (souvent absent) : auteur ou média\"},\n",
    "\n",
    "  {\"pred\":\"schema:worksFor\",\n",
    "   \"subj\":[\"foaf:Person\"], \"obj\":[\"schema:Organization\"],\n",
    "   \"note\":\"Emploi / affiliation forte\"},\n",
    "\n",
    "  {\"pred\":\"schema:founder\",\n",
    "   \"subj\":[\"schema:Organization\"], \"obj\":[\"foaf:Person\"],\n",
    "   \"note\":\"Organisation -> fondateur(s)\"},\n",
    "\n",
    "  {\"pred\":\"schema:acquiredBy\",\n",
    "   \"subj\":[\"schema:Organization\"], \"obj\":[\"schema:Organization\"],\n",
    "   \"note\":\"Convention: acquis -> acquéreur\"},\n",
    "\n",
    "  {\"pred\":\"schema:produces\",\n",
    "   \"subj\":[\"schema:Organization\"], \"obj\":[\"schema:Product\"],\n",
    "   \"note\":\"Entreprise -> produit (hardware/software)\"},\n",
    "\n",
    "  {\"pred\":\"schema:location\",\n",
    "   \"subj\":[\"schema:Organization\",\"schema:Event\"], \"obj\":[\"schema:Place\"],\n",
    "   \"note\":\"Si une localisation est explicitement mentionnée\"},\n",
    "\n",
    "  {\"pred\":\"onto:announced\",\n",
    "   \"subj\":[\"schema:Organization\",\"foaf:Person\"], \"obj\":[\"schema:Product\",\"schema:Event\"],\n",
    "   \"note\":\"Local (si tu veux expliciter les annonces/lancements)\"},\n",
    "\n",
    "  {\"pred\":\"schema:datePublished\",\n",
    "   \"subj\":[\"schema:NewsArticle\"], \"obj\":[\"xsd:date\",\"xsd:dateTime\"],\n",
    "   \"note\":\"Attribut date article (si dispo / inférée)\"},\n",
    "\n",
    "  {\"pred\":\"schema:releaseDate\",\n",
    "   \"subj\":[\"schema:Product\",\"schema:Event\"], \"obj\":[\"xsd:date\",\"xsd:dateTime\"],\n",
    "   \"note\":\"Attribut date sortie/lancement (si mentionné)\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_TYPES = {\n",
    "  \"schema:worksFor\":   ({\"PERSON\"}, {\"ORG\"}),\n",
    "  \"schema:founder\":    ({\"ORG\"}, {\"PERSON\"}),\n",
    "  \"schema:acquiredBy\": ({\"ORG\"}, {\"ORG\"}),\n",
    "  \"schema:produces\":   ({\"ORG\"}, {\"PRODUCT\"}),\n",
    "  \"schema:location\":   ({\"ORG\",\"EVENT\"}, {\"GPE\"}),\n",
    "  \"onto:announced\":    ({\"ORG\",\"PERSON\"}, {\"PRODUCT\",\"EVENT\"}),\n",
    "  # about/author/datePublished/releaseDate : laisse plus souple pour l’instant\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c13319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Dict, Any, Tuple\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# Détection \"best effort\" des colonnes\n",
    "TEXT_COL = next((c for c in [\"Text\", \"text\", \"News\", \"news\", \"Content\", \"content\"] if c in df_tech.columns), None)\n",
    "if TEXT_COL is None:\n",
    "    raise KeyError(f\"Colonne texte introuvable. Colonnes dispo: {list(df_tech.columns)}\")\n",
    "\n",
    "ID_COL = next((c for c in [\"id\", \"Id\", \"ID\", \"article_id\", \"ArticleId\"] if c in df_tech.columns), None)\n",
    "\n",
    "ALLOWED_PREDS = [x[\"pred\"] for x in REL_SCHEMA]\n",
    "\n",
    "CACHE_DIR = Path(\"cache_extractions\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c8be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "EntityType = Literal[\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"EVENT\"]\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    mention: str = Field(min_length=1)\n",
    "    type: EntityType\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    subj: str = Field(min_length=1)\n",
    "    pred: str = Field(min_length=1)\n",
    "    obj: str = Field(min_length=1)\n",
    "    evidence: str = \"\"\n",
    "\n",
    "class Extraction(BaseModel):\n",
    "    entities: List[Entity] = []\n",
    "    relations: List[Relation] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s*```$\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _extract_first_json_object(s: str) -> str:\n",
    "    # essaie de récupérer le premier bloc {...} si le modèle a bavé\n",
    "    s = _strip_code_fences(s)\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return s\n",
    "    return s[start:end+1]\n",
    "\n",
    "def _find_span(text: str, mention: str):\n",
    "    if not mention:\n",
    "        return -1, -1, mention\n",
    "    # cherche case-insensitive mais renvoie les indices exacts\n",
    "    m = re.search(re.escape(mention), text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return -1, -1, mention\n",
    "    real = text[m.start():m.end()]  # substring exact du texte (casse d'origine)\n",
    "    return m.start(), m.end(), real\n",
    "\n",
    "def _postprocess(extr: Extraction, text: str) -> Extraction:\n",
    "    # offsets\n",
    "    fixed_entities = []\n",
    "    for e in extr.entities:\n",
    "        s, t, real = _find_span(text, e.mention)\n",
    "        fixed_entities.append(Entity(mention=real, type=e.type, start=s, end=t))\n",
    "    extr.entities = fixed_entities\n",
    "\n",
    "\n",
    "    entity_mentions = {e.mention for e in extr.entities}\n",
    "\n",
    "    # relations: pred whitelist + subj/obj exist + evidence substring\n",
    "    fixed_rel = []\n",
    "    for r in extr.relations:\n",
    "        if r.pred not in ALLOWED_PREDS:\n",
    "            continue\n",
    "        if (r.subj not in entity_mentions) or (r.obj not in entity_mentions and r.pred not in [\"schema:datePublished\", \"schema:releaseDate\"]):\n",
    "            # pour les dates, obj peut être un literal ISO. Sinon on exige obj dans entities.\n",
    "            continue\n",
    "        if r.evidence and (r.evidence not in text):\n",
    "            r.evidence = \"\"\n",
    "        fixed_rel.append(r)\n",
    "\n",
    "    extr.relations = fixed_rel\n",
    "    return extr\n",
    "\n",
    "# ============================================================\n",
    "# A) Corriger les mentions : récupérer la casse réelle + dédoublonner\n",
    "# ============================================================\n",
    "\n",
    "def find_span_case_insensitive(text: str, mention: str) -> Tuple[int, int, str]:\n",
    "    \"\"\"Trouve la mention dans le texte (case-insensitive) et retourne la casse réelle.\"\"\"\n",
    "    if not mention:\n",
    "        return -1, -1, mention\n",
    "    m = re.search(re.escape(mention), text, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return -1, -1, mention\n",
    "    real = text[m.start():m.end()]  # substring exact (casse d'origine)\n",
    "    return m.start(), m.end(), real\n",
    "\n",
    "def dedup_entities(entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Dédoublonne les entités par (type, start, end) si start!=-1, sinon (type, mention.lower()).\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for e in entities:\n",
    "        key = (e[\"type\"], e[\"start\"], e[\"end\"]) if e[\"start\"] != -1 else (e[\"type\"], e[\"mention\"].lower())\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(e)\n",
    "    return out\n",
    "\n",
    "def fix_entities_from_text(extr: Dict[str, Any], text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Corrige les mentions (casse réelle) et dédoublonne les entités.\"\"\"\n",
    "    ents = []\n",
    "    for e in extr.get(\"entities\", []):\n",
    "        s, t, real = find_span_case_insensitive(text, e[\"mention\"])\n",
    "        ents.append({**e, \"mention\": real, \"start\": s, \"end\": t})\n",
    "    extr[\"entities\"] = dedup_entities(ents)\n",
    "    return extr\n",
    "\n",
    "# ============================================================\n",
    "# B) Nettoyer / retourner les relations + vérifier l'evidence\n",
    "# ============================================================\n",
    "\n",
    "def clean_relations(extr: Dict[str, Any], text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Nettoie les relations :\n",
    "    - Retourne automatiquement les relations inversées (ex: worksFor Person→Org)\n",
    "    - Rejette les relations dont les types ne matchent pas le schéma\n",
    "    - Exige que l'evidence contienne les deux mentions (subj+obj)\n",
    "    \"\"\"\n",
    "    types = {e[\"mention\"]: e[\"type\"] for e in extr.get(\"entities\", [])}\n",
    "    mentions = set(types.keys())\n",
    "\n",
    "    def has_both_in_evidence(subj: str, obj: str, ev: str) -> bool:\n",
    "        if not ev:\n",
    "            return False\n",
    "        # case-insensitive containment\n",
    "        return (re.search(re.escape(subj), ev, re.IGNORECASE) is not None) and \\\n",
    "               (re.search(re.escape(obj), ev, re.IGNORECASE) is not None)\n",
    "\n",
    "    cleaned = []\n",
    "    for r in extr.get(\"relations\", []):\n",
    "        pred = r.get(\"pred\")\n",
    "        subj = r.get(\"subj\")\n",
    "        obj  = r.get(\"obj\")\n",
    "        ev   = r.get(\"evidence\", \"\") or \"\"\n",
    "\n",
    "        if pred not in ALLOWED_PREDS:\n",
    "            continue\n",
    "        if subj not in mentions:\n",
    "            continue\n",
    "        if pred not in [\"schema:datePublished\", \"schema:releaseDate\"] and obj not in mentions:\n",
    "            continue\n",
    "\n",
    "        # evidence doit être substring du texte (sinon vide)\n",
    "        if ev and ev not in text:\n",
    "            ev = \"\"\n",
    "\n",
    "        # typing + auto-swap si inversé\n",
    "        if pred in PRED_TYPES and obj in mentions:\n",
    "            subj_ok, obj_ok = PRED_TYPES[pred]\n",
    "            st = types.get(subj)\n",
    "            ot = types.get(obj)\n",
    "\n",
    "            # swap si inversé\n",
    "            if (st in obj_ok) and (ot in subj_ok):\n",
    "                subj, obj = obj, subj\n",
    "                st, ot = ot, st\n",
    "\n",
    "            if not (st in subj_ok and ot in obj_ok):\n",
    "                continue\n",
    "\n",
    "            # evidence doit contenir les 2 mentions (sinon on drop)\n",
    "            if not has_both_in_evidence(subj, obj, ev):\n",
    "                continue\n",
    "\n",
    "        cleaned.append({\"subj\": subj, \"pred\": pred, \"obj\": obj, \"evidence\": ev})\n",
    "\n",
    "    extr[\"relations\"] = cleaned\n",
    "    return extr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46352bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"llama3.1:8b\"\n",
    "\n",
    "def _ollama_chat(messages: List[Dict[str, str]], temperature: float = 0.0, timeout: int = 120) -> str:\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": temperature}\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data[\"message\"][\"content\"]\n",
    "\n",
    "def _build_prompt(article_text: str) -> List[Dict[str, str]]:\n",
    "    system = (\n",
    "        \"Tu es un extracteur d'information. \"\n",
    "        \"Tu DOIS répondre en JSON strict, sans aucun texte autour, sans markdown. \"\n",
    "        \"Le JSON doit respecter EXACTEMENT ce schéma: \"\n",
    "        \"{\\\"entities\\\":[{\\\"mention\\\":str,\\\"type\\\":\\\"PERSON|ORG|GPE|PRODUCT|EVENT\\\",\\\"start\\\":int,\\\"end\\\":int}],\"\n",
    "        \"\\\"relations\\\":[{\\\"subj\\\":str,\\\"pred\\\":str,\\\"obj\\\":str,\\\"evidence\\\":str}]}. \"\n",
    "        f\"pred doit être dans cette liste exacte: {ALLOWED_PREDS}. \"\n",
    "        \"Règles: \"\n",
    "        \"1) Les champs entities/relations existent toujours (liste vide ok). \"\n",
    "        \"2) subj et obj doivent correspondre à des 'mention' présentes dans entities (sauf pred datePublished/releaseDate où obj peut être une date ISO). \"\n",
    "        \"3) evidence doit être un extrait COPIÉ du texte source (30-200 chars si possible). \"\n",
    "        \"4) N'invente rien: si doute -> n'extrais pas.\"\n",
    "    )\n",
    "\n",
    "    user = (\n",
    "        \"Texte:\\n\"\n",
    "        f\"{article_text}\\n\\n\"\n",
    "        \"Retourne UNIQUEMENT le JSON.\"\n",
    "    )\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "def extract(article_text: str, article_id: str | int, max_retries: int = 3) -> Dict[str, Any]:\n",
    "    cache_path = CACHE_DIR / f\"{article_id}.jsonl\"\n",
    "    if cache_path.exists():\n",
    "        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.loads(f.readline())\n",
    "\n",
    "    messages = _build_prompt(article_text)\n",
    "\n",
    "    last_err = None\n",
    "    raw = \"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        raw = _ollama_chat(messages, temperature=0.0)\n",
    "        raw_json = _extract_first_json_object(raw)\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw_json)\n",
    "            extr = Extraction.model_validate(parsed)\n",
    "            extr = _postprocess(extr, article_text)\n",
    "\n",
    "            out = extr.model_dump()\n",
    "            # cache (jsonl = une ligne)\n",
    "            with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "            return out\n",
    "\n",
    "        except (json.JSONDecodeError, ValidationError) as e:\n",
    "            last_err = str(e)\n",
    "            # retry: demander une correction JSON stricte\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Corrige la sortie pour qu'elle soit du JSON strict conforme au schéma. Réponds en JSON uniquement.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"ERREUR:\\n{last_err}\\n\\nSORTIE A CORRIGER:\\n{raw}\\n\\nRappel: pred doit être dans {ALLOWED_PREDS}.\"}\n",
    "            ]\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # si échec total: on cache un résultat vide (évite boucles)\n",
    "    fallback = {\"entities\": [], \"relations\": [], \"error\": last_err, \"raw\": raw[:1000]}\n",
    "    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(fallback, ensure_ascii=False) + \"\\n\")\n",
    "    return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in tqdm(df_tech.iterrows(), total=len(df_tech)):\n",
    "    article_id = row[ID_COL] if ID_COL else i\n",
    "    text = str(row[TEXT_COL])\n",
    "\n",
    "    res = extract(text, article_id=article_id)\n",
    "    \n",
    "    # Post-traitement : correction casse + déduplication + nettoyage relations\n",
    "    res = fix_entities_from_text(res, text)\n",
    "    res = clean_relations(res, text)\n",
    "    \n",
    "    results.append({\"article_id\": article_id, **res})\n",
    "\n",
    "results[0].keys(), len(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
